{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by Piyush Sanghi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Data Processing: \n",
    "This data set is a bit messy, so the preprocessing portion is largely a tutorial to make sure students have data ready for keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a) Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "import numpy\n",
    "import optparse\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b) We will read the code in slightly differently than before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pandas.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c) We then need to convert to a numpy.ndarray type: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"timestamp\":1502738402847,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"timestamp\":1502738402849,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"timestamp\":1502738402852,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"timestamp\":1502738402852,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"timestamp\":1502738402853,\"method\":\"post\",\"qu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  {\"timestamp\":1502738402847,\"method\":\"post\",\"qu...  0\n",
       "1  {\"timestamp\":1502738402849,\"method\":\"post\",\"qu...  0\n",
       "2  {\"timestamp\":1502738402852,\"method\":\"post\",\"qu...  0\n",
       "3  {\"timestamp\":1502738402852,\"method\":\"post\",\"qu...  0\n",
       "4  {\"timestamp\":1502738402853,\"method\":\"post\",\"qu...  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d) Check the shape of the data set - it should be (26773, 2). Spend some time looking at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### e) Store all rows and the 0th index as the feature data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### f) Store all rows and index 1 as the target variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### g) In the next step, we will clean up the predictors. This includes removing features that are not valuable, such as timestamp and source. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### h) We next will tokenize our data, which just means vectorizing our text. Given the data we will tokenize every character (thus char_level = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index)+1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### i) Need to pad our data as each observation has a different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### j) Create your train set to be 75% of the data and your test set to be 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.shuffle(X_processed)\n",
    "X_train, X_test = X_processed[:int(0.75*X_processed.shape[0]),], X_processed[int(0.75*X_processed.shape[0]):,]\n",
    "y_train, y_test = Y[:int(0.75*Y.shape[0]),], Y[int(0.75*Y.shape[0]):,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26773, 1024), (20079, 1024), (6694, 1024), (20079,), (6694,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed.shape, X_train.shape, X_test.shape, y_train.shape, y_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model 1 - RNN: \n",
    "The first model will be a pretty minimal RNN with only an embedding layer, simple RNN and Dense layer. The next model we will add a few more layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a) Start by creating an instance of a Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b) From there, add an Embedding layer: https://keras.io/layers/embeddings/ (Links to an external site.)Links to an external site.\n",
    "\n",
    "Params:\n",
    "- input_dim = num_words (the variable we created above)\n",
    "- output_dim = 32\n",
    "- input_length = max_log_length (we also created this above) \n",
    "- Keep all other variables as the defaults (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(input_dim=num_words, output_dim=32, input_length=max_log_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c) Add a SimpleRNN layer: https://keras.io/layers/recurrent/ (Links to an external site.)Links to an external site.\n",
    "\n",
    "Params:\n",
    "- units = 32\n",
    "- activation = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(SimpleRNN(units=32, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d) Finally, we will add a Dense layer: https://keras.io/layers/core/#dense (Links to an external site.)Links to an external site.\n",
    "\n",
    "Params:\n",
    "- units = 1 (this will be our output)\n",
    "- activation = sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### e) Compile model using the .compile() method: https://keras.io/models/model/ (Links to an external site.)Links to an external site.\n",
    "\n",
    "Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### f) Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,129\n",
      "Trainable params: 4,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### g) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 117s 8ms/step - loss: 0.6799 - acc: 0.5822 - val_loss: 0.7587 - val_acc: 0.3400\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 116s 8ms/step - loss: 0.6797 - acc: 0.5822 - val_loss: 0.7557 - val_acc: 0.3400\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 115s 8ms/step - loss: 0.6797 - acc: 0.5822 - val_loss: 0.7488 - val_acc: 0.3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2205ad17b70>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### h) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 25s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7204432913998472, 0.4390498954287422]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Model 2 - RNN + Dropout Layers + New Activation Function:\n",
    "\n",
    "Now we will add a few new layers to our RNN and incorporate the more powerful LSTM. You will be creating a new model here, so make sure to call it something different than the model from Part 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a) This RNN needs to have the following layers (add in this order):\n",
    "\n",
    "- Embedding Layer (use same params as before)\n",
    "- LSTM Layer (units = 64, recurrent_dropout = 0.5)\n",
    "- Dropout Layer - use a value of 0.5 \n",
    "- Dense Layer - (use same params as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=num_words, output_dim=32, input_length=max_log_length))\n",
    "model2.add(LSTM(units=64, recurrent_dropout=0.5))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b) Compile model using the .compile() method:\n",
    "    Params:\n",
    "- loss = binary_crossentropy\n",
    "- optimizer = adam\n",
    "- metrics = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c) Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/1\n",
      "15059/15059 [==============================] - 115s 8ms/step - loss: 0.6798 - acc: 0.5822 - val_loss: 0.7568 - val_acc: 0.3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22055d95c18>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 49s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6934225755304873, 0.44801314610209897]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Recurrent Neural Net Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### a) RNN Requirements: \n",
    "- Use 5 or more layers\n",
    "- Add a layer that was not utilized in Model 1 or Model 2 (Note: This could be a new Dense layer or an additional LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(input_dim=num_words, output_dim=32, input_length=max_log_length))\n",
    "model3.add(LSTM(units=64, recurrent_dropout=.05))\n",
    "model3.add(Dense(units=64, activation='relu'))\n",
    "model3.add(Dropout(0.4))\n",
    "model3.add(Dense(units=64, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(units=64, activation='relu'))\n",
    "model3.add(Dropout(0.3))\n",
    "model3.add(Dense(units=64, activation='relu'))\n",
    "model3.add(Dense(units=1, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### b) Compiler Requirements: \n",
    "- Try a new optimizer for the compile step\n",
    "- Keep accuracy as a metric (feel free to add more metrics if desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### c) Print the model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 43,553\n",
      "Trainable params: 43,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### d) Use the .fit() method to fit the model on the train data. Use a validation split of 0.25, epochs=3 and batch size = 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15059 samples, validate on 5020 samples\n",
      "Epoch 1/3\n",
      "15059/15059 [==============================] - 293s 19ms/step - loss: 0.7560 - acc: 0.5569 - val_loss: 0.7638 - val_acc: 0.3400\n",
      "Epoch 2/3\n",
      "15059/15059 [==============================] - 287s 19ms/step - loss: 0.6865 - acc: 0.5711 - val_loss: 0.7700 - val_acc: 0.3400\n",
      "Epoch 3/3\n",
      "15059/15059 [==============================] - 286s 19ms/step - loss: 0.6828 - acc: 0.5797 - val_loss: 0.7858 - val_acc: 0.3400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220761ca160>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of 128 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6694/6694 [==============================] - 49s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7428522710661978, 0.4390498954287422]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.metrics_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
